{"Name":"GPUImage","Id":2344,"Alias":"GPUImage","Description":"\n\u003ciframe src=\"https://appetize.io/embed/4nxnxqc6hd3k5289n7q5dt3hm8?device=iphone5s\u0026scale=75\u0026autoplay=true\u0026orientation=portrait\u0026deviceColor=black\" \n        width=\"274px\" height=\"587px\" frameborder=\"0\" scrolling=\"no\"\n        style=\"float:right;margin-left:1em;\"\u003e\u0026nbsp;\u003c/iframe\u003e\n\n**GPUImage** lets you apply GPU-accelerated filters and other effects to images, \nlive camera video, and movies. In comparison to Core Image (part of iOS 5.0), \nGPUImage allows you to write your own custom filters, supports deployment to iOS \n4.0, and has a simpler interface. However, it currently lacks some of the more \nadvanced features of Core Image, such as facial detection.\n\nFor massively parallel operations like processing images or live video frames, \nGPUs have some significant performance advantages over CPUs. On an iPhone 4, a \nsimple image filter can be over 100 times faster to perform on the GPU than an \nequivalent CPU-based filter.\n\nHowever, running custom filters on the GPU requires a lot of code to set up and \nmaintain an OpenGL ES 2.0 rendering target for these filters. I created a [sample \nproject to do this][1], and found that there was a lot of boilerplate code I had \nto write in its creation. Therefore, I put together this framework that \nencapsulates a lot of the common tasks you\u0027ll encounter when processing images \nand video and made it so that you don\u0027t need to care about the OpenGL ES 2.0 \nunderpinnings.\n\nThis framework compares favorably to Core Image when handling video, taking only \n2.5 ms on an iPhone 4 to upload a frame from the camera, apply a gamma filter, \nand display, versus 106 ms for the same operation using Core Image. CPU-based \nprocessing takes 460 ms, making GPUImage 40X faster than Core Image for this \noperation on this hardware, and 184X faster than CPU-bound processing. On an \niPhone 4S, GPUImage is only 4X faster than Core Image for this case, and 102X \nfaster than CPU-bound processing. However, for more complex operations like \nGaussian blurs at larger radii, Core Image currently outpaces GPUImage.\n\n## General Architecture\n\nGPUImage uses OpenGL ES 2.0 shaders to perform image and video manipulation much \nfaster than could be done in CPU-bound routines. However, it hides the complexity \nof interacting with the OpenGL ES API in a simplified interface. This \ninterface lets you define input sources for images and video, attach filters in \na chain, and send the resulting processed image or video to the screen, to a \n`UIImage`, or to a movie on disk.\n\nImages or frames of video are uploaded from source objects, which are subclasses \nof `GPUImageOutput`. These include `GPUImageVideoCamera` (for live video from an \niOS camera), `GPUImageStillCamera` (for taking photos with the camera), \n`GPUImagePicture` (for still images), and `GPUImageMovie` (for movies). Source \nobjects upload still image frames to OpenGL ES as textures, then hand those \ntextures off to the next objects in the processing chain.\n\nFilters and other subsequent elements in the chain conform to the `GPUImageInput` \nprotocol, which lets them take in the supplied or processed texture from the \nprevious link in the chain and do something with it. Objects one step further \ndown the chain are considered targets, and processing can be branched by adding \nmultiple targets to a single output or filter.\n\nFor example, an application that takes in live video from the camera, converts \nthat video to a sepia tone, then displays the video onscreen would set up a \nchain looking something like the following:\n\n    GPUImageVideoCamera -\u003e GPUImageSepiaFilter -\u003e GPUImageView\n\n## Performing Common Tasks\n\n### Filtering Live Video\n\nTo filter live video from an iOS device\u0027s camera, you can use code like the \nfollowing:\n\n    videoCamera = new GPUImageVideoCamera (\n        AVCaptureSession.Preset640x480,\n        AVCaptureDevicePosition.Back);\n    videoCamera.OutputImageOrientation = InterfaceOrientation;\n    \n    filter = GPUImageFilter.FromFragmentShaderFile (\"CustomShader\");\n    videoCamera.AddTarget (filter);\n    \n    videoView = new GPUImageView ();\n    filter.AddTarget (imageView);\n    \n    videoCamera.StartCameraCapture ();\n\nThis sets up a video source coming from the iOS device\u0027s back-facing camera, \nusing a preset that tries to capture at 640x480. This video is captured with \nthe interface being in portrait mode, where the landscape-left-mounted camera \nneeds to have its video frames rotated before display. A custom filter, using \ncode from the file CustomShader.fsh, is then set as the target for the video \nframes from the camera. These filtered video frames are finally displayed \nonscreen with the help of a `UIView` subclass that can present the filtered \nOpenGL ES texture that results from this pipeline.\n\nThe fill mode of the `GPUImageView` can be altered by setting its `FillMode` \nproperty, so that if the aspect ratio of the source video is different from \nthat of the view, the video will either be stretched, centered with black \nbars, or zoomed to fill.\n\nFor blending filters and others that take in more than one image, you can \ncreate multiple outputs and add a single filter as a target for both of \nthese outputs. The order with which the outputs are added as targets will \naffect the order in which the input images are blended or otherwise processed.\n\nAlso, if you wish to enable microphone audio capture for recording to a movie, \nyou\u0027ll need to set the `AudioEncodingTarget` of the camera to be your movie writer, \nlike for the following:\n\n    videoCamera.AudioEncodingTarget = movieWriter;\n\n### Capturing \u0026 Filtering Still Photo\n\nTo capture and filter still photos, you can use a process similar to the one for \nfiltering video. Instead of a `GPUImageVideoCamera`, you use a \n`GPUImageStillCamera`:\n\n    stillCamera = new GPUImageStillCamera ();\n    stillCamera.OutputImageOrientation = InterfaceOrientation;\n    \n    filter = new GPUImageGammaFilter ();\n    stillCamera.AddTarget (filter);\n    \n    photoView = new GPUImageView ();\n    filter.AddTarget (photoView);\n    \n    stillCamera.StartCameraCapture ();\n\nThis will give you a live, filtered feed of the still camera\u0027s preview video. \nNote that this preview video is only provided on iOS 4.3 and higher, so you may \nneed to set that as your deployment target if you wish to have this functionality.\n\nOnce you want to capture a photo, you use a callback or the async method:\n\n    NSData processedJPEG = await stillCamera.CapturePhotoAsJPEGAsync (filter);\n    \n    var library = new ALAssetsLibrary ();\n    var assetURL = await library.WriteImageToSavedPhotosAlbumAsync (\n        processedJPEG, stillCamera.CurrentCaptureMetadata);\n\t\nThe above code captures a full-size photo processed by the same filter chain used \nin the preview view and saves that photo to disk as a JPEG in the device\u0027s asset\nlibrary.\n\nNote that the framework currently can\u0027t handle images larger than 2048 pixels wide \nor high on older devices (those before the iPhone 4S, iPad 2, or Retina iPad) due \nto texture size limitations. This means that the iPhone 4, whose camera outputs \nstill photos larger than this, won\u0027t be able to capture photos like this. A \ntiling mechanism is being implemented to work around this. All other devices \nshould be able to capture and filter photos using this method.\n\n### Processing Still Images\n\nThere are a couple of ways to process a still image and create a result. The \nfirst way you can do this is by creating a still image source object and manually \ncreating a filter chain:\n\n    inputImage = UIImage.FromBundle (\"Lambeau.jpg\");\n    \n    imageSource = new GPUImagePicture (inputImage);\n    sepiaFilter = new GPUImageSepiaFilter ();\n    \n    imageSource.AddTarget (sepiaFilter);\n    sepiaFilter.UseNextFrameForImageCapture ();\n    await imageSource.ProcessImageAsync ();\n    \n    UIImage currentImage = vignetteFilter.ToImage ()\n\nNote that for a manual capture of an image from a filter, you need to set \n`UseNextFrameForImageCapture` in order to tell the filter that you\u0027ll be needing \nto capture from it later. By default, GPUImage reuses framebuffers within \nfilters to conserve memory, so if you need to hold on to a filter\u0027s framebuffer \nfor manual image capture, you need to let it know ahead of time. \n\nFor single filters that you wish to apply to an image, you can simply do the \nfollowing:\n\n    inputImage = UIImage.FromBundle (\"Lambeau.jpg\");\n    \n    imageFilter = new GPUImageSketchFilter ();\n    UIImage quickFilteredImage = imageFilter.CreateFilteredImage (inputImage);\n\n\n### Writing Custom Filter\n\nOne significant advantage of this framework over Core Image on iOS (as of iOS 5.0) \nis the ability to write your own custom image and video processing filters. These \nfilters are supplied as OpenGL ES 2.0 fragment shaders, written in the C-like \nOpenGL Shading Language. \n\nA custom filter is initialized with code like\n\n    filter = GPUImageFilter.FromFragmentShaderFile (\"CustomShader\");\n\nwhere the extension used for the fragment shader is .fsh. Additionally, \nyou can use the `FromFragmentShaderString` static method to provide the fragment \nshader as a string, if you would not like to ship your fragment shaders in your \napplication bundle.\n\nFragment shaders perform their calculations for each pixel to be rendered at \nthat filter stage. They do this using the OpenGL Shading Language (GLSL), a \nC-like language with additions specific to 2-D and 3-D graphics. An example \nof a fragment shader is the following sepia-tone filter:\n\n    varying highp vec2 textureCoordinate;\n    uniform sampler2D inputImageTexture;\n    \n    void main()\n    {\n        lowp vec4 textureColor = texture2D(inputImageTexture, textureCoordinate);\n        lowp vec4 outputColor;\n        outputColor.r = (textureColor.r * 0.393) + (textureColor.g * 0.769) + (textureColor.b * 0.189);\n        outputColor.g = (textureColor.r * 0.349) + (textureColor.g * 0.686) + (textureColor.b * 0.168);    \n        outputColor.b = (textureColor.r * 0.272) + (textureColor.g * 0.534) + (textureColor.b * 0.131);\n        outputColor.a = 1.0;\n        \n        gl_FragColor = outputColor;\n    }\n\nFor an image filter to be usable within the GPUImage framework, the first two \nlines that take in the textureCoordinate varying (for the current coordinate \nwithin the texture, normalized to 1.0) and the inputImageTexture uniform (for \nthe actual input image frame texture) are required.\n\nThe remainder of the shader grabs the color of the pixel at this location in \nthe passed-in texture, manipulates it in such a way as to produce a sepia tone, \nand writes that pixel color out to be used in the next stage of the processing \npipeline.\n\n### Filtering \u0026 Re-Encoding Movies\n\nMovies can be loaded into the framework via the `GPUImageMovie` class, filtered, \nand then written out using a `GPUImageMovieWriter`. `GPUImageMovieWriter` is \nalso fast enough to record video in realtime from an iPhone 4\u0027s camera at 640x480, \nso a direct filtered video source can be fed into it. \n\nCurrently, `GPUImageMovieWriter` is fast enough to record live 720p video at up \nto 20 FPS on the iPhone 4, and both 720p and 1080p video at 30 FPS on the \niPhone 4S (as well as on the new iPad).\n\nThe following is an example of how you would load a sample movie, pass it \nthrough a pixellation filter, then record the result to disk as a 480 x 640 \nh.264 movie:\n\n    movieFile = new GPUImageMovie (sampleURL);\n    filter = new GPUImagePixellateFilter ();\n    \n    movieFile.AddTarget (filter);\n    \n    var documents = Environment.GetFolderPath (Environment.SpecialFolder.MyDocuments);\n    var pathToMovie = Path.Combine (documents, \"Movie.m4v\");\n    if (File.Exists (pathToMovie)) {\n        File.Delete (pathToMovie);\n    }\n    var movieURL = new NSUrl (pathToMovie, false);\n    \n    movieWriter = new GPUImageMovieWriter (movieURL, new CGSize (640.0f, 480.0f));\n    filter.AddTarget (movieWriter);\n    \n    movieWriter.ShouldPassthroughAudio = true;\n    movieFile.AudioEncodingTarget = movieWriter;\n    \n    movieFile.EnableSynchronizedEncoding (movieWriter);\n    \n    movieWriter.StartRecording ();\n    movieFile.StartProcessing ();\n\nOnce recording is finished, you need to remove the movie recorder from the \nfilter chain and close off the recording using code like the following:\n\n    movieWriter.CompletionHandler = async () =\u003e {\n        filter.RemoveTarget (movieWriter);\n        await movieWriter.FinishRecordingAsync ();\n    };\n\nA movie won\u0027t be usable until it has been finished off, so if this is \ninterrupted before this point, the recording will be lost.\n\n### Interacting with OpenGL ES\n\nGPUImage can both export and import textures from OpenGL ES through the use of \nits `GPUImageTextureOutput` and `GPUImageTextureInput` classes, respectively. This \nlets you record a movie from an OpenGL ES scene that is rendered to a framebuffer \nobject with a bound texture, or filter video or images and then feed them into \nOpenGL ES as a texture to be displayed in the scene.\n\nThe one caution with this approach is that the textures used in these processes \nmust be shared between GPUImage\u0027s OpenGL ES context and any other context via a \nshare group or something similar.\n\n## Built-in Filters\n\nThere are currently 125 built-in filters, divided into the following categories:\n\n### Color Adjustments\n\n- **GPUImageBrightnessFilter**: Adjusts the brightness of the image\n- **GPUImageExposureFilter**: Adjusts the exposure of the image\n- **GPUImageContrastFilter**: Adjusts the contrast of the image\n- **GPUImageSaturationFilter**: Adjusts the saturation of an image\n- **GPUImageGammaFilter**: Adjusts the gamma of an image\n- **GPUImageLevelsFilter**: Photoshop-like levels adjustment. The min, max, minOut \n  and maxOut parameters are floats in the range [0, 1]. \n- **GPUImageColorMatrixFilter**: Transforms the colors of an image by applying a \n  matrix to them\n- **GPUImageRGBFilter**: Adjusts the individual RGB channels of an image\n- **GPUImageHueFilter**: Adjusts the hue of an image\n- **GPUImageWhiteBalanceFilter**: Adjusts the white balance of an image.\n- **GPUImageToneCurveFilter**: Adjusts the colors of an image based on spline \n  curves for each color channel.\n- **GPUImageHighlightShadowFilter**: Adjusts the shadows and highlights of an \n  image\n- **GPUImageLookupFilter**: Uses an RGB color lookup image to remap the colors \n  in an image. ([lookup.png][lookup])\n- **GPUImageAmatorkaFilter**: A photo filter based on a [Photoshop action by \n  Amatorka][2]. ([lookup_amatorka.png][lookup_amatorka])\n- **GPUImageMissEtikateFilter**: A photo filter based on a [Photoshop action by \n  Miss Etikate][3]. ([lookup_miss_etikate.png][lookup_miss_etikate])\n- **GPUImageSoftEleganceFilter**: Another lookup-based color remapping filter. \n  ([lookup_soft_elegance_1.png][lookup_soft_elegance_1] and [lookup_soft_elegance_2.png][lookup_soft_elegance_2])\n- **GPUImageColorInvertFilter**: Inverts the colors of an image\n- **GPUImageGrayscaleFilter**: Converts an image to grayscale (a slightly faster \n  implementation of the saturation filter, without the ability to vary the \n  color contribution)\n- **GPUImageMonochromeFilter**: Converts the image to a single-color version, \n  based on the luminance of each pixel\n- **GPUImageFalseColorFilter**: Uses the luminance of the image to mix between \n  two user-specified colors\n- **GPUImageHazeFilter**: Used to add or remove haze (similar to a UV filter)\n- **GPUImageSepiaFilter**: Simple sepia tone filter\n- **GPUImageOpacityFilter**: Adjusts the alpha channel of the incoming image\n- **GPUImageSolidColorGenerator**: This outputs a generated image with a solid \n  color.\n- **GPUImageLuminanceThresholdFilter**: Pixels with a luminance above the \n  threshold will appear white, and those below will be black\n- **GPUImageAdaptiveThresholdFilter**: Determines the local luminance around a \n  pixel, then turns the pixel black if it is below that local luminance and \n  white if above.\n- **GPUImageAverageLuminanceThresholdFilter**: This applies a thresholding \n  operation where the threshold is continually adjusted based on the average \n  luminance of the scene.\n- **GPUImageHistogramFilter**: This analyzes the incoming image and creates \n  an output histogram with the frequency at which each color value occurs. \n- **GPUImageHistogramGenerator**: This is a special filter, in that it\u0027s \n  primarily intended to work with the `GPUImageHistogramFilter`. It generates \n  an output representation of the color histograms generated by \n  `GPUImageHistogramFilter`.\n- **GPUImageAverageColor**: This processes an input image and determines the \n  average color of the scene, by averaging the RGBA components for each \n  pixel in the image.\n- **GPUImageLuminosity**: Like the `GPUImageAverageColor`, this reduces an image \n  to its average luminosity.\n- **GPUImageChromaKeyFilter**: For a given color in the image, sets the alpha \n  channel to 0.\n\n### Image Processing\n\n- **GPUImageTransformFilter**: This applies an arbitrary 2-D or 3-D transformation \n  to an image\n- **GPUImageCropFilter**: This crops an image to a specific region, then passes \n  only that region on to the next stage in the filter\n- **GPUImageLanczosResamplingFilter**: This lets you up- or downsample an image \n  using Lanczos resampling\n- **GPUImageSharpenFilter**: Sharpens the image\n- **GPUImageUnsharpMaskFilter**: Applies an unsharp mask\n- **GPUImageGaussianBlurFilter**: A hardware-optimized, variable-radius Gaussian \n  blur\n- **GPUImageBoxBlurFilter**: A hardware-optimized, variable-radius box blur\n- **GPUImageSingleComponentGaussianBlurFilter**: A modification of the \n  `GPUImageGaussianBlurFilter` that operates only on the red component\n- **GPUImageGaussianSelectiveBlurFilter**: A Gaussian blur that preserves \n  focus within a circular region\n- **GPUImageGaussianBlurPositionFilter**: The inverse of the \n  `GPUImageGaussianSelectiveBlurFilter`, applying the blur only within a certain \n  circle\n- **GPUImageiOSBlurFilter**: An attempt to replicate the background blur used on \n  iOS 7 in places like the control center.\n- **GPUImageMedianFilter**: Takes the median value of the three color components, \n  over a 3x3 area\n- **GPUImageBilateralFilter**: A bilateral blur, which tries to blur similar color \n  values while preserving sharp edges\n- **GPUImageTiltShiftFilter**: A simulated tilt shift lens effect\n- **GPUImage3x3ConvolutionFilter**: Runs a 3x3 convolution kernel against the \n  image\n- **GPUImageSobelEdgeDetectionFilter**: Sobel edge detection, with edges \n  highlighted in white\n- **GPUImagePrewittEdgeDetectionFilter**: Prewitt edge detection, with edges \n  highlighted in white\n- **GPUImageThresholdEdgeDetectionFilter**: Performs Sobel edge detection, but \n  applies a threshold instead of giving gradual strength values\n- **GPUImageCannyEdgeDetectionFilter**: This uses the full Canny process to \n  highlight one-pixel-wide edges\n- **GPUImageHarrisCornerDetectionFilter**: Runs the Harris corner detection \n  algorithm on an input image, and produces an image with those corner points \n  as white pixels and everything else black.\n- **GPUImageNobleCornerDetectionFilter**: Runs the Noble variant on the Harris \n  corner detector. It behaves as described above for the Harris detector.\n- **GPUImageShiTomasiCornerDetectionFilter**: Runs the Shi-Tomasi feature \n  detector. It behaves as described above for the Harris detector.\n- **GPUImageNonMaximumSuppressionFilter**: Currently used only as part of the \n  Harris corner detection filter\n- **GPUImageXYDerivativeFilter**: An internal component within the Harris corner \n  detection filter\n- **GPUImageCrosshairGenerator**: This draws a series of crosshairs on an image, \n  most often used for identifying machine vision features.\n- **GPUImageDilationFilter**: This performs an image dilation operation, where \n  the maximum intensity of the red channel in a rectangular neighborhood is \n  used for the intensity of this pixel.\n- **GPUImageRGBDilationFilter**: This is the same as the `GPUImageDilationFilter`, \n  except that this acts on all color channels, not just the red channel.\n- **GPUImageErosionFilter**: This performs an image erosion operation, where the \n  minimum intensity of the red channel in a rectangular neighborhood is used for \n  the intensity of this pixel.\n- **GPUImageRGBErosionFilter**: This is the same as the `GPUImageErosionFilter`, \n  except that this acts on all color channels, not just the red channel.\n- **GPUImageOpeningFilter**: This performs an erosion on the red channel of an \n  image, followed by a dilation of the same radius.\n- **GPUImageRGBOpeningFilter**: This is the same as the `GPUImageOpeningFilter`, \n  except that this acts on all color channels, not just the red channel.\n- **GPUImageClosingFilter**: This performs a dilation on the red channel of an \n  image, followed by an erosion of the same radius.\n- **GPUImageRGBClosingFilter**: This is the same as the `GPUImageClosingFilter`, \n  except that this acts on all color channels, not just the red channel.\n- **GPUImageLocalBinaryPatternFilter**: This performs a comparison of intensity \n  of the red channel of the 8 surrounding pixels and that of the central one, \n  encoding the comparison results in a bit string that becomes this pixel \n  intensity.\n- **GPUImageLowPassFilter**: This applies a low pass filter to incoming video \n  frames.\n- **GPUImageHighPassFilter**: This applies a high pass filter to incoming video \n  frames.\n- **GPUImageMotionDetector**: This is a motion detector based on a high-pass \n  filter.\n- **GPUImageHoughTransformLineDetector**: Detects lines in the image using a \n  Hough transform into parallel coordinate space.\n- **GPUImageLineGenerator**: A helper class that generates lines which can \n  overlay the scene.\n- **GPUImageMotionBlurFilter**: Applies a directional motion blur to an image\n- **GPUImageZoomBlurFilter**: Applies a directional motion blur to an image\n\n### Blending Modes\n\n- **GPUImageChromaKeyBlendFilter**: Selectively replaces a color in the first \n  image with the second image\n- **GPUImageDissolveBlendFilter**: Applies a dissolve blend of two images\n- **GPUImageMultiplyBlendFilter**: Applies a multiply blend of two images\n- **GPUImageAddBlendFilter**: Applies an additive blend of two images\n- **GPUImageSubtractBlendFilter**: Applies a subtractive blend of two images\n- **GPUImageDivideBlendFilter**: Applies a division blend of two images\n- **GPUImageOverlayBlendFilter**: Applies an overlay blend of two images\n- **GPUImageDarkenBlendFilter**: Blends two images by taking the minimum value \n  of each color component between the images\n- **GPUImageLightenBlendFilter**: Blends two images by taking the maximum value \n  of each color component between the images\n- **GPUImageColorBurnBlendFilter**: Applies a color burn blend of two images\n- **GPUImageColorDodgeBlendFilter**: Applies a color dodge blend of two images\n- **GPUImageScreenBlendFilter**: Applies a screen blend of two images\n- **GPUImageExclusionBlendFilter**: Applies an exclusion blend of two images\n- **GPUImageDifferenceBlendFilter**: Applies a difference blend of two images\n- **GPUImageHardLightBlendFilter**: Applies a hard light blend of two images\n- **GPUImageSoftLightBlendFilter**: Applies a soft light blend of two images\n- **GPUImageAlphaBlendFilter**: Blends the second image over the first, based \n  on the second\u0027s alpha channel\n- **GPUImageSourceOverBlendFilter**: Applies a source over blend of two images\n- **GPUImageColorBurnBlendFilter**: Applies a color burn blend of two images\n- **GPUImageColorDodgeBlendFilter**: Applies a color dodge blend of two images\n- **GPUImageNormalBlendFilter**: Applies a normal blend of two images\n- **GPUImageColorBlendFilter**: Applies a color blend of two images\n- **GPUImageHueBlendFilter**: Applies a hue blend of two images\n- **GPUImageSaturationBlendFilter**: Applies a saturation blend of two images\n- **GPUImageLuminosityBlendFilter**: Applies a luminosity blend of two images\n- **GPUImageLinearBurnBlendFilter**: Applies a linear burn blend of two images\n- **GPUImagePoissonBlendFilter**: Applies a Poisson blend of two images\n- **GPUImageMaskFilter**: Masks one image using another\n\n### Visual Effects\n\n- **GPUImagePixellateFilter**: Applies a pixellation effect on an image or video\n- **GPUImagePolarPixellateFilter**: Applies a pixellation effect on an image or \n  video, based on polar coordinates instead of Cartesian ones\n- **GPUImagePolkaDotFilter**: Breaks an image up into colored dots within a \n  regular grid\n- **GPUImageHalftoneFilter**: Applies a halftone effect to an image, like \n  news print\n- **GPUImageCrosshatchFilter**: This converts an image into a black-and-white \n  crosshatch pattern\n- **GPUImageSketchFilter**: Converts video to look like a sketch. This is just \n  the Sobel edge detection filter with the colors inverted\n- **GPUImageThresholdSketchFilter**: Same as the sketch filter, only the edges \n  are thresholded instead of being grayscale\n- **GPUImageToonFilter**: This uses Sobel edge detection to place a black border \n  around objects, and then it quantizes the colors present in the image to give \n  a cartoon-like quality to the image.\n- **GPUImageSmoothToonFilter**: This uses a similar process as the \n  `GPUImageToonFilter`, only it precedes the toon effect with a Gaussian blur \n  to smooth out noise.\n- **GPUImageEmbossFilter**: Applies an embossing effect on the image\n- **GPUImagePosterizeFilter**: This reduces the color dynamic range into the \n  number of steps specified, leading to a cartoon-like simple shading of the \n  image.\n- **GPUImageSwirlFilter**: Creates a swirl distortion on the image\n- **GPUImageBulgeDistortionFilter**: Creates a bulge distortion on the image\n- **GPUImagePinchDistortionFilter**: Creates a pinch distortion of the image\n- **GPUImageStretchDistortionFilter**: Creates a stretch distortion of the image\n- **GPUImageSphereRefractionFilter**: Simulates the refraction through a glass \n  sphere\n- **GPUImageGlassSphereFilter**: Same as the `GPUImageSphereRefractionFilter`, only \n  the image is not inverted and there\u0027s a little bit of frosting at the edges of \n  the glass\n- **GPUImageVignetteFilter**: Performs a vignetting effect, fading out the \n  image at the edges\n- **GPUImageKuwaharaFilter**: Kuwahara image abstraction. This produces an oil-\n  painting-like image, but it is extremely computationally expensive\n- **GPUImageKuwaharaRadius3Filter**: A modified version of the Kuwahara filter, \n  optimized to work over just a radius of three pixels\n- **GPUImagePerlinNoiseFilter**: Generates an image full of Perlin noise\n- **GPUImageCGAColorspaceFilter**: Simulates the colorspace of a CGA monitor\n- **GPUImageMosaicFilter**: This filter takes an input tileset, the tiles must \n  ascend in luminance.\n- **GPUImageJFAVoronoiFilter**: Generates a Voronoi map, for use in a later stage.\n- **GPUImageVoronoiConsumerFilter**: Takes in the Voronoi map, and uses that \n  to filter an incoming image.\n\nYou can also easily write your own custom filters using the C-like OpenGL \nShading Language, as described above.\n\n[1]: http://www.sunsetlakesoftware.com/2010/10/22/gpu-accelerated-video-processing-mac-and-ios\n[2]: http://amatorka.deviantart.com/art/Amatorka-Action-2-121069631\n[3]: http://miss-etikate.deviantart.com/art/Photoshop-Action-15-120151961\n[4]: http://medusa.fit.vutbr.cz/public/data/papers/2011-SCCG-Dubska-Real-Time-Line-Detection-Using-PC-and-OpenGL.pdf\n[5]: http://medusa.fit.vutbr.cz/public/data/papers/2011-CVPR-Dubska-PClines.pdf\n\n[lookup]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup.png\n[lookup_amatorka]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup_amatorka.png\n[lookup_miss_etikate]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup_miss_etikate.png\n[lookup_soft_elegance_1]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup_soft_elegance_1.png\n[lookup_soft_elegance_2]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup_soft_elegance_2.png\n","Version":"0.1.7.0","Summary":"An open source iOS framework for GPU-based image and video processing.","QuickStart":"\n**GPUImage** lets you apply GPU-accelerated filters and other effects to images, \nlive camera video, and movies. In comparison to Core Image (part of iOS 5.0), \nGPUImage allows you to write your own custom filters, supports deployment to iOS \n4.0, and has a simpler interface. However, it currently lacks some of the more \nadvanced features of Core Image, such as facial detection.\n\nFor massively parallel operations like processing images or live video frames, \nGPUs have some significant performance advantages over CPUs. On an iPhone 4, a \nsimple image filter can be over 100 times faster to perform on the GPU than an \nequivalent CPU-based filter.\n\nHowever, running custom filters on the GPU requires a lot of code to set up and \nmaintain an OpenGL ES 2.0 rendering target for these filters. I created a [sample \nproject to do this][1], and found that there was a lot of boilerplate code I had \nto write in its creation. Therefore, I put together this framework that \nencapsulates a lot of the common tasks you\u0027ll encounter when processing images \nand video and made it so that you don\u0027t need to care about the OpenGL ES 2.0 \nunderpinnings.\n\nThis framework compares favorably to Core Image when handling video, taking only \n2.5 ms on an iPhone 4 to upload a frame from the camera, apply a gamma filter, \nand display, versus 106 ms for the same operation using Core Image. CPU-based \nprocessing takes 460 ms, making GPUImage 40X faster than Core Image for this \noperation on this hardware, and 184X faster than CPU-bound processing. On an \niPhone 4S, GPUImage is only 4X faster than Core Image for this case, and 102X \nfaster than CPU-bound processing. However, for more complex operations like \nGaussian blurs at larger radii, Core Image currently outpaces GPUImage.\n\n## Technical Requirements\n\n- OpenGL ES 2.0: Applications using this will not run on the original iPhone, \n  iPhone 3G, and 1st and 2nd generation iPod touches\n- iOS 4.1 as a deployment target (4.0 didn\u0027t have some extensions needed for \n  movie reading). iOS 4.3 is needed as a deployment target if you wish to show \n  live video previews when taking a still photo.\n- iOS 5.0 SDK to build\n- Devices must have a camera to use camera-related functionality (obviously)\n\n\n## General Architecture\n\nGPUImage uses OpenGL ES 2.0 shaders to perform image and video manipulation much \nfaster than could be done in CPU-bound routines. However, it hides the complexity \nof interacting with the OpenGL ES API in a simplified interface. This \ninterface lets you define input sources for images and video, attach filters in \na chain, and send the resulting processed image or video to the screen, to a \n`UIImage`, or to a movie on disk.\n\nImages or frames of video are uploaded from source objects, which are subclasses \nof `GPUImageOutput`. These include `GPUImageVideoCamera` (for live video from an \niOS camera), `GPUImageStillCamera` (for taking photos with the camera), \n`GPUImagePicture` (for still images), and `GPUImageMovie` (for movies). Source \nobjects upload still image frames to OpenGL ES as textures, then hand those \ntextures off to the next objects in the processing chain.\n\nFilters and other subsequent elements in the chain conform to the `GPUImageInput` \nprotocol, which lets them take in the supplied or processed texture from the \nprevious link in the chain and do something with it. Objects one step further \ndown the chain are considered targets, and processing can be branched by adding \nmultiple targets to a single output or filter.\n\nFor example, an application that takes in live video from the camera, converts \nthat video to a sepia tone, then displays the video onscreen would set up a \nchain looking something like the following:\n\n    GPUImageVideoCamera -\u003e GPUImageSepiaFilter -\u003e GPUImageView\n\n## Performing Common Tasks\n\n### Filtering Live Video\n\nTo filter live video from an iOS device\u0027s camera, you can use code like the \nfollowing:\n\n    videoCamera = new GPUImageVideoCamera (\n        AVCaptureSession.Preset640x480,\n        AVCaptureDevicePosition.Back);\n    videoCamera.OutputImageOrientation = InterfaceOrientation;\n    \n    filter = GPUImageFilter.FromFragmentShaderFile (\"CustomShader\");\n    videoCamera.AddTarget (filter);\n    \n    videoView = new GPUImageView ();\n    filter.AddTarget (imageView);\n    \n    videoCamera.StartCameraCapture ();\n\nThis sets up a video source coming from the iOS device\u0027s back-facing camera, \nusing a preset that tries to capture at 640x480. This video is captured with \nthe interface being in portrait mode, where the landscape-left-mounted camera \nneeds to have its video frames rotated before display. A custom filter, using \ncode from the file CustomShader.fsh, is then set as the target for the video \nframes from the camera. These filtered video frames are finally displayed \nonscreen with the help of a `UIView` subclass that can present the filtered \nOpenGL ES texture that results from this pipeline.\n\nThe fill mode of the `GPUImageView` can be altered by setting its `FillMode` \nproperty, so that if the aspect ratio of the source video is different from \nthat of the view, the video will either be stretched, centered with black \nbars, or zoomed to fill.\n\nFor blending filters and others that take in more than one image, you can \ncreate multiple outputs and add a single filter as a target for both of \nthese outputs. The order with which the outputs are added as targets will \naffect the order in which the input images are blended or otherwise processed.\n\nAlso, if you wish to enable microphone audio capture for recording to a movie, \nyou\u0027ll need to set the `AudioEncodingTarget` of the camera to be your movie writer, \nlike for the following:\n\n    videoCamera.AudioEncodingTarget = movieWriter;\n\n\n### Capturing \u0026 Filtering Still Photo\n\nTo capture and filter still photos, you can use a process similar to the one for \nfiltering video. Instead of a `GPUImageVideoCamera`, you use a \n`GPUImageStillCamera`:\n\n    stillCamera = new GPUImageStillCamera ();\n    stillCamera.OutputImageOrientation = InterfaceOrientation;\n    \n    filter = new GPUImageGammaFilter ();\n    stillCamera.AddTarget (filter);\n    \n    photoView = new GPUImageView ();\n    filter.AddTarget (photoView);\n    \n    stillCamera.StartCameraCapture ();\n\nThis will give you a live, filtered feed of the still camera\u0027s preview video. \nNote that this preview video is only provided on iOS 4.3 and higher, so you may \nneed to set that as your deployment target if you wish to have this functionality.\n\nOnce you want to capture a photo, you use a callback or the async method:\n\n    NSData processedJPEG = await stillCamera.CapturePhotoAsJPEGAsync (filter);\n    \n    var library = new ALAssetsLibrary ();\n    var assetURL = await library.WriteImageToSavedPhotosAlbumAsync (\n        processedJPEG, stillCamera.CurrentCaptureMetadata);\n\t\nThe above code captures a full-size photo processed by the same filter chain used \nin the preview view and saves that photo to disk as a JPEG in the device\u0027s asset\nlibrary.\n\nNote that the framework currently can\u0027t handle images larger than 2048 pixels wide \nor high on older devices (those before the iPhone 4S, iPad 2, or Retina iPad) due \nto texture size limitations. This means that the iPhone 4, whose camera outputs \nstill photos larger than this, won\u0027t be able to capture photos like this. A \ntiling mechanism is being implemented to work around this. All other devices \nshould be able to capture and filter photos using this method.\n\n### Processing Still Images\n\nThere are a couple of ways to process a still image and create a result. The \nfirst way you can do this is by creating a still image source object and manually \ncreating a filter chain:\n\n    inputImage = UIImage.FromBundle (\"Lambeau.jpg\");\n    \n    imageSource = new GPUImagePicture (inputImage);\n    sepiaFilter = new GPUImageSepiaFilter ();\n    \n    imageSource.AddTarget (sepiaFilter);\n    sepiaFilter.UseNextFrameForImageCapture ();\n    await imageSource.ProcessImageAsync ();\n    \n    UIImage currentImage = vignetteFilter.ToImage ()\n\nNote that for a manual capture of an image from a filter, you need to set \n`UseNextFrameForImageCapture` in order to tell the filter that you\u0027ll be needing \nto capture from it later. By default, GPUImage reuses framebuffers within \nfilters to conserve memory, so if you need to hold on to a filter\u0027s framebuffer \nfor manual image capture, you need to let it know ahead of time. \n\nFor single filters that you wish to apply to an image, you can simply do the \nfollowing:\n\n    inputImage = UIImage.FromBundle (\"Lambeau.jpg\");\n    \n    imageFilter = new GPUImageSketchFilter ();\n    UIImage quickFilteredImage = imageFilter.CreateFilteredImage (inputImage);\n\n\n### Writing Custom Filter\n\nOne significant advantage of this framework over Core Image on iOS (as of iOS 5.0) \nis the ability to write your own custom image and video processing filters. These \nfilters are supplied as OpenGL ES 2.0 fragment shaders, written in the C-like \nOpenGL Shading Language. \n\nA custom filter is initialized with code like\n\n    filter = GPUImageFilter.FromFragmentShaderFile (\"CustomShader\");\n\nwhere the extension used for the fragment shader is .fsh. Additionally, \nyou can use the `FromFragmentShaderString` static method to provide the fragment \nshader as a string, if you would not like to ship your fragment shaders in your \napplication bundle.\n\nFragment shaders perform their calculations for each pixel to be rendered at \nthat filter stage. They do this using the OpenGL Shading Language (GLSL), a \nC-like language with additions specific to 2-D and 3-D graphics. An example \nof a fragment shader is the following sepia-tone filter:\n\n    varying highp vec2 textureCoordinate;\n    uniform sampler2D inputImageTexture;\n    \n    void main()\n    {\n        lowp vec4 textureColor = texture2D(inputImageTexture, textureCoordinate);\n        lowp vec4 outputColor;\n        outputColor.r = (textureColor.r * 0.393) + (textureColor.g * 0.769) + (textureColor.b * 0.189);\n        outputColor.g = (textureColor.r * 0.349) + (textureColor.g * 0.686) + (textureColor.b * 0.168);    \n        outputColor.b = (textureColor.r * 0.272) + (textureColor.g * 0.534) + (textureColor.b * 0.131);\n        outputColor.a = 1.0;\n        \n        gl_FragColor = outputColor;\n    }\n\nFor an image filter to be usable within the GPUImage framework, the first two \nlines that take in the textureCoordinate varying (for the current coordinate \nwithin the texture, normalized to 1.0) and the inputImageTexture uniform (for \nthe actual input image frame texture) are required.\n\nThe remainder of the shader grabs the color of the pixel at this location in \nthe passed-in texture, manipulates it in such a way as to produce a sepia tone, \nand writes that pixel color out to be used in the next stage of the processing \npipeline.\n\n### Filtering \u0026 Re-Encoding Movies\n\nMovies can be loaded into the framework via the `GPUImageMovie` class, filtered, \nand then written out using a `GPUImageMovieWriter`. `GPUImageMovieWriter` is \nalso fast enough to record video in realtime from an iPhone 4\u0027s camera at 640x480, \nso a direct filtered video source can be fed into it. \n\nCurrently, `GPUImageMovieWriter` is fast enough to record live 720p video at up \nto 20 FPS on the iPhone 4, and both 720p and 1080p video at 30 FPS on the \niPhone 4S (as well as on the new iPad).\n\nThe following is an example of how you would load a sample movie, pass it \nthrough a pixellation filter, then record the result to disk as a 480 x 640 \nh.264 movie:\n\n    movieFile = new GPUImageMovie (sampleURL);\n    filter = new GPUImagePixellateFilter ();\n    \n    movieFile.AddTarget (filter);\n    \n    var documents = Environment.GetFolderPath (Environment.SpecialFolder.MyDocuments);\n    var pathToMovie = Path.Combine (documents, \"Movie.m4v\");\n    if (File.Exists (pathToMovie)) {\n        File.Delete (pathToMovie);\n    }\n    var movieURL = new NSUrl (pathToMovie, false);\n    \n    movieWriter = new GPUImageMovieWriter (movieURL, new CGSize (640.0f, 480.0f));\n    filter.AddTarget (movieWriter);\n    \n    movieWriter.ShouldPassthroughAudio = true;\n    movieFile.AudioEncodingTarget = movieWriter;\n    \n    movieFile.EnableSynchronizedEncoding (movieWriter);\n    \n    movieWriter.StartRecording ();\n    movieFile.StartProcessing ();\n\nOnce recording is finished, you need to remove the movie recorder from the \nfilter chain and close off the recording using code like the following:\n\n    movieWriter.CompletionHandler = async () =\u003e {\n        filter.RemoveTarget (movieWriter);\n        await movieWriter.FinishRecordingAsync ();\n    };\n\nA movie won\u0027t be usable until it has been finished off, so if this is \ninterrupted before this point, the recording will be lost.\n\n### Interacting with OpenGL ES\n\nGPUImage can both export and import textures from OpenGL ES through the use of \nits `GPUImageTextureOutput` and `GPUImageTextureInput` classes, respectively. This \nlets you record a movie from an OpenGL ES scene that is rendered to a framebuffer \nobject with a bound texture, or filter video or images and then feed them into \nOpenGL ES as a texture to be displayed in the scene.\n\nThe one caution with this approach is that the textures used in these processes \nmust be shared between GPUImage\u0027s OpenGL ES context and any other context via a \nshare group or something similar.\n\n## Built-in Filters\n\nThere are currently 125 built-in filters, divided into the following categories:\n\n### Color Adjustments\n\n- **GPUImageBrightnessFilter**: Adjusts the brightness of the image\n  - *brightness*: The adjusted brightness (-1.0 - 1.0, with 0.0 as the default)\n\n- **GPUImageExposureFilter**: Adjusts the exposure of the image\n  - *exposure*: The adjusted exposure (-10.0 - 10.0, with 0.0 as the default)\n\n- **GPUImageContrastFilter**: Adjusts the contrast of the image\n  - *contrast*: The adjusted contrast (0.0 - 4.0, with 1.0 as the default)\n\n- **GPUImageSaturationFilter**: Adjusts the saturation of an image\n  - *saturation*: The degree of saturation or desaturation to apply to the image \n    (0.0 - 2.0, with 1.0 as the default)\n\n- **GPUImageGammaFilter**: Adjusts the gamma of an image\n  - *gamma*: The gamma adjustment to apply (0.0 - 3.0, with 1.0 as the default)\n\n- **GPUImageLevelsFilter**: Photoshop-like levels adjustment. The min, max, minOut \n  and maxOut parameters are floats in the range [0, 1]. If you have parameters \n  from Photoshop in the range [0, 255] you must first convert them to be [0, 1]. \n  The gamma/mid parameter is a float \u003e= 0. This matches the value from Photoshop. \n  If you want to apply levels to RGB as well as individual channels you need to \n  use this filter twice - first for the individual channels and then for all \n  channels.\n\n- **GPUImageColorMatrixFilter**: Transforms the colors of an image by applying a \n  matrix to them\n  - *colorMatrix*: A 4x4 matrix used to transform each color in an image\n  - *intensity*: The degree to which the new transformed color replaces the \n    original color for each pixel\n\n- **GPUImageRGBFilter**: Adjusts the individual RGB channels of an image\n  - *red*: Normalized values by which each color channel is multiplied. \n    The range is from 0.0 up, with 1.0 as the default.\n  - *green*: \n  - *blue*:\n\n- **GPUImageHueFilter**: Adjusts the hue of an image\n  - *hue*: The hue angle, in degrees. 90 degrees by default\n\n- **GPUImageWhiteBalanceFilter**: Adjusts the white balance of an image.\n  - *temperature*: The temperature to adjust the image by, in ºK. A value of \n    4000 is very cool and 7000 very warm. The default value is 5000. Note that \n    the scale between 4000 and 5000 is nearly as visually significant as that \n    between 5000 and 7000.\n  - *tint*: The tint to adjust the image by. A value of -200 is *very* green \n    and 200 is *very* pink. The default value is 0.  \n\n- **GPUImageToneCurveFilter**: Adjusts the colors of an image based on spline \n  curves for each color channel.\n  - *redControlPoints*:\n  - *greenControlPoints*:\n  - *blueControlPoints*: \n  - *rgbCompositeControlPoints*: The tone curve takes in a series of control \n    points that define the spline curve for each color component, or for all \n    three in the composite. These are stored as `NSValue`-wrapped `CGPoint`s in \n    an `NSArray`, with normalized X and Y coordinates from 0 - 1. The defaults \n    are (0,0), (0.5,0.5), (1,1).\n\n- **GPUImageHighlightShadowFilter**: Adjusts the shadows and highlights of an \n  image\n  - *shadows*: Increase to lighten shadows, from 0.0 to 1.0, with 0.0 as the \n    default.\n  - *highlights*: Decrease to darken highlights, from 0.0 to 1.0, with 1.0 \n    as the default.\n\n- **GPUImageLookupFilter**: Uses an RGB color lookup image to remap the colors \n  in an image. First, use your favourite photo editing application to apply a \n  filter to [lookup.png][lookup] from the GPUImage resources folder. For this \n  to work properly each pixel color must not depend on other pixels (e.g. blur \n  will not work). If you need a more complex filter you can create as many lookup \n  tables as required. Once ready, use your new lookup.png file as a second \n  input for `GPUImageLookupFilter`.\n\n- **GPUImageAmatorkaFilter**: A photo filter based on a [Photoshop action by \n  Amatorka][2]. \n  If you want to use this effect you have to add [lookup_amatorka.png][lookup_amatorka]\n  from the GPUImage resources folder to your application bundle.\n\n- **GPUImageMissEtikateFilter**: A photo filter based on a [Photoshop action by \n  Miss Etikate][3].\n  If you want to use this effect you have to add [lookup_miss_etikate.png][lookup_miss_etikate]\n  from the GPUImage resources folder to your application bundle.\n\n- **GPUImageSoftEleganceFilter**: Another lookup-based color remapping filter. \n  If you want to use this effect you have to add [lookup_soft_elegance_1.png][lookup_soft_elegance_1]\n  and [lookup_soft_elegance_2.png][lookup_soft_elegance_2] from the GPUImage \n  resources folder to your application bundle.\n\n- **GPUImageColorInvertFilter**: Inverts the colors of an image\n\n- **GPUImageGrayscaleFilter**: Converts an image to grayscale (a slightly faster \n  implementation of the saturation filter, without the ability to vary the \n  color contribution)\n\n- **GPUImageMonochromeFilter**: Converts the image to a single-color version, \n  based on the luminance of each pixel\n  - *intensity*: The degree to which the specific color replaces the normal \n    image color (0.0 - 1.0, with 1.0 as the default)\n  - *color*: The color to use as the basis for the effect, with (0.6, 0.45, \n    0.3, 1.0) as the default.\n\n- **GPUImageFalseColorFilter**: Uses the luminance of the image to mix between \n  two user-specified colors\n  - *firstColor*: The first and second colors specify what colors replace the \n    dark and light areas of the image, respectively. The defaults are (0.0, \n    0.0, 0.5) amd (1.0, 0.0, 0.0).\n  - *secondColor*: \n\n- **GPUImageHazeFilter**: Used to add or remove haze (similar to a UV filter)\n  - *distance*: Strength of the color applied. Default 0. Values between -.3 \n    and .3 are best.\n  - *slope*: Amount of color change. Default 0. Values between -.3 and .3 \n    are best.\n\n- **GPUImageSepiaFilter**: Simple sepia tone filter\n  - *intensity*: The degree to which the sepia tone replaces the normal image \n    color (0.0 - 1.0, with 1.0 as the default)\n\n- **GPUImageOpacityFilter**: Adjusts the alpha channel of the incoming image\n  - *opacity*: The value to multiply the incoming alpha channel for each pixel \n    by (0.0 - 1.0, with 1.0 as the default)\n\n- **GPUImageSolidColorGenerator**: This outputs a generated image with a solid \n  color. You need to define the image size using `ForceProcessingAtSize`\n  - *color*: The color, in a four component format, that is used to fill the \n    image.\n\n- **GPUImageLuminanceThresholdFilter**: Pixels with a luminance above the \n  threshold will appear white, and those below will be black\n  - *threshold*: The luminance threshold, from 0.0 to 1.0, with a default of 0.5\n\n- **GPUImageAdaptiveThresholdFilter**: Determines the local luminance around a \n  pixel, then turns the pixel black if it is below that local luminance and \n  white if above. This can be useful for picking out text under varying \n  lighting conditions.\n  - *blurRadiusInPixels*: A multiplier for the background averaging blur radius \n    in pixels, with a default of 4.\n\n- **GPUImageAverageLuminanceThresholdFilter**: This applies a thresholding \n  operation where the threshold is continually adjusted based on the average \n  luminance of the scene.\n  - *thresholdMultiplier*: This is a factor that the average luminance will \n    be multiplied by in order to arrive at the final threshold to use. By \n    default, this is 1.0.\n\n- **GPUImageHistogramFilter**: This analyzes the incoming image and creates \n  an output histogram with the frequency at which each color value occurs. \n  The output of this filter is a 3-pixel-high, 256-pixel-wide image with \n  the center (vertical) pixels containing pixels that correspond to the frequency \n  at which various color values occurred. Each color value occupies one of the \n  256 width positions, from 0 on the left to 255 on the right. This histogram \n  can be generated for individual color channels (Red, Green, Blue), the \n  luminance of the image (Luminance), or for all three color channels at \n  once (Rgb).\n  - *downsamplingFactor*: Rather than sampling every pixel, this dictates \n    what fraction of the image is sampled. By default, this is 16 with a \n    minimum of 1. This is needed to keep from saturating the histogram, which \n    can only record 256 pixels for each color value before it becomes overloaded.\n\n- **GPUImageHistogramGenerator**: This is a special filter, in that it\u0027s \n  primarily intended to work with the `GPUImageHistogramFilter`. It generates \n  an output representation of the color histograms generated by \n  `GPUImageHistogramFilter`, but it could be repurposed to display other \n  kinds of values. It takes in an image and looks at the center (vertical) \n  pixels. It then plots the numerical values of the RGB components in separate \n  colored graphs in an output texture. You may need to force a size for this \n  filter in order to make its output visible.\n\n- **GPUImageAverageColor**: This processes an input image and determines the \n  average color of the scene, by averaging the RGBA components for each \n  pixel in the image. A reduction process is used to progressively downsample \n  the source image on the GPU, followed by a short averaging calculation on \n  the CPU. The output from this filter is meaningless, but you need to set the \n  `ColorAverageProcessingFinishedHandler` property to a delegate that takes in \n  four color components and a frame time and does something with them.\n\n- **GPUImageLuminosity**: Like the `GPUImageAverageColor`, this reduces an image \n  to its average luminosity. You need to set the \n  `LuminosityProcessingFinishedHandler` to handle the output of this filter, \n  which just returns a luminosity value and a frame time.\n\n- **GPUImageChromaKeyFilter**: For a given color in the image, sets the alpha \n  channel to 0. This is similar to the `GPUImageChromaKeyBlendFilter`, only \n  instead of blending in a second image for a matching color this doesn\u0027t take \n  in a second image and just turns a given color transparent.\n  - *thresholdSensitivity*: How close a color match needs to exist to the target \n    color to be replaced (default of 0.4)\n  - *smoothing*: How smoothly to blend for the color match (default of 0.1)\n\n### Image Processing\n\n- **GPUImageTransformFilter**: This applies an arbitrary 2-D or 3-D transformation \n  to an image\n  - *affineTransform*: This takes in a CGAffineTransform to adjust an image in 2-D\n  - *transform3D*: This takes in a CATransform3D to manipulate an image in 3-D\n  - *ignoreAspectRatio*: By default, the aspect ratio of the transformed image \n    is maintained, but this can be set to YES to make the transformation \n    independent of aspect ratio\n\n- **GPUImageCropFilter**: This crops an image to a specific region, then passes \n  only that region on to the next stage in the filter\n  - *cropRegion*: A rectangular area to crop out of the image, normalized to \n    coordinates from 0.0 - 1.0. The (0.0, 0.0) position is in the upper left \n    of the image.\n\n- **GPUImageLanczosResamplingFilter**: This lets you up- or downsample an image \n  using Lanczos resampling, which results in noticeably better quality than the \n  standard linear or trilinear interpolation. Simply use `ForceProcessingAtSize` \n  to set the target output resolution for the filter, and the image will be \n  resampled for that new size.\n\n- **GPUImageSharpenFilter**: Sharpens the image\n  - *sharpness*: The sharpness adjustment to apply (-4.0 - 4.0, with 0.0 as the \n    default)\n\n- **GPUImageUnsharpMaskFilter**: Applies an unsharp mask\n  - *blurRadiusInPixels*: The blur radius of the underlying Gaussian blur. The \n    default is 4.0.\n  - *intensity*: The strength of the sharpening, from 0.0 on up, with a default \n    of 1.0\n\n- **GPUImageGaussianBlurFilter**: A hardware-optimized, variable-radius Gaussian \n  blur\n  - *texelSpacingMultiplier*: A multiplier for the spacing between texels, \n    ranging from 0.0 on up, with a default of 1.0. Adjusting this may slightly \n    increase the blur strength, but will introduce artifacts in the result. \n    Highly recommend using other parameters first, before touching this one.\n  - *blurRadiusInPixels*: A radius in pixels to use for the blur, with a default \n    of 2.0. This adjusts the sigma variable in the Gaussian distribution function.\n  - *blurRadiusAsFractionOfImageWidth*: \n  - *blurRadiusAsFractionOfImageHeight*: Setting these properties will allow \n    the blur radius to scale with the size of the image\n  - *blurPasses*: The number of times to sequentially blur the incoming image. \n    The more passes, the slower the filter.\n\n- **GPUImageBoxBlurFilter**: A hardware-optimized, variable-radius box blur\n  - *texelSpacingMultiplier*: A multiplier for the spacing between texels, \n    ranging from 0.0 on up, with a default of 1.0. Adjusting this may slightly \n    increase the blur strength, but will introduce artifacts in the result. \n    Highly recommend using other parameters first, before touching this one.\n  - *blurRadiusInPixels*: A radius in pixels to use for the blur, with a default \n    of 2.0. This adjusts the sigma variable in the Gaussian distribution function.\n  - *blurRadiusAsFractionOfImageWidth*: \n  - *blurRadiusAsFractionOfImageHeight*: Setting these properties will allow the \n    blur radius to scale with the size of the image\n  - *blurPasses*: The number of times to sequentially blur the incoming image. \n    The more passes, the slower the filter.\n\n- **GPUImageSingleComponentGaussianBlurFilter**: A modification of the \n  `GPUImageGaussianBlurFilter` that operates only on the red component\n  - *texelSpacingMultiplier*: A multiplier for the spacing between texels, \n    ranging from 0.0 on up, with a default of 1.0. Adjusting this may slightly \n    increase the blur strength, but will introduce artifacts in the result. \n    Highly recommend using other parameters first, before touching this one.\n  - *blurRadiusInPixels*: A radius in pixels to use for the blur, with a \n    default of 2.0. This adjusts the sigma variable in the Gaussian \n    distribution function.\n  - *blurRadiusAsFractionOfImageWidth*: \n  - *blurRadiusAsFractionOfImageHeight*: Setting these properties will \n    allow the blur radius to scale with the size of the image\n  - *blurPasses*: The number of times to sequentially blur the incoming \n    image. The more passes, the slower the filter.\n\n- **GPUImageGaussianSelectiveBlurFilter**: A Gaussian blur that preserves \n  focus within a circular region\n  - *blurRadiusInPixels*: A radius in pixels to use for the blur, with a \n    default of 5.0. This adjusts the sigma variable in the Gaussian \n    distribution function.\n  - *excludeCircleRadius*: The radius of the circular area being excluded \n    from the blur\n  - *excludeCirclePoint*: The center of the circular area being excluded from \n    the blur\n  - *excludeBlurSize*: The size of the area between the blurred portion and \n    the clear circle \n  - *aspectRatio*: The aspect ratio of the image, used to adjust the circularity \n    of the in-focus region. By default, this matches the image aspect ratio, \n    but you can override this value.\n\n- **GPUImageGaussianBlurPositionFilter**: The inverse of the \n  `GPUImageGaussianSelectiveBlurFilter`, applying the blur only within a certain \n  circle\n  - *blurSize*: A multiplier for the size of the blur, ranging from 0.0 on up, \n    with a default of 1.0\n  - *blurCenter*: Center for the blur, defaults to 0.5, 0.5\n  - *blurRadius*: Radius for the blur, defaults to 1.0\n\n- **GPUImageiOSBlurFilter**: An attempt to replicate the background blur used on \n  iOS 7 in places like the control center.\n  - *blurRadiusInPixels*: A radius in pixels to use for the blur, with a default \n    of 12.0. This adjusts the sigma variable in the Gaussian distribution function.\n  - *saturation*: Saturation ranges from 0.0 (fully desaturated) to 2.0 (max \n    saturation), with 0.8 as the normal level\n  - *downsampling*: The degree to which to downsample, then upsample the incoming \n    image to minimize computations within the Gaussian blur, with a default of \n    4.0.\n\n- **GPUImageMedianFilter**: Takes the median value of the three color components, \n  over a 3x3 area\n\n- **GPUImageBilateralFilter**: A bilateral blur, which tries to blur similar color \n  values while preserving sharp edges\n  - *texelSpacingMultiplier*: A multiplier for the spacing between texel reads, \n    ranging from 0.0 on up, with a default of 4.0\n  - *distanceNormalizationFactor*: A normalization factor for the distance \n    between central color and sample color, with a default of 8.0.\n\n- **GPUImageTiltShiftFilter**: A simulated tilt shift lens effect\n  - *blurRadiusInPixels*: The radius of the underlying blur, in pixels. This \n    is 7.0 by default.\n  - *topFocusLevel*: The normalized location of the top of the in-focus area \n    in the image, this value should be lower than bottomFocusLevel, default 0.4\n  - *bottomFocusLevel*: The normalized location of the bottom of the in-focus \n    area in the image, this value should be higher than topFocusLevel, default 0.6\n  - *focusFallOffRate*: The rate at which the image gets blurry away from the \n    in-focus region, default 0.2\n\n- **GPUImage3x3ConvolutionFilter**: Runs a 3x3 convolution kernel against the \n  image\n  - *convolutionKernel*: The convolution kernel is a 3x3 matrix of values to \n    apply to the pixel and its 8 surrounding pixels. The matrix is specified \n    in row-major order, with the top left pixel being one.one and the bottom \n    right three.three. If the values in the matrix don\u0027t add up to 1.0, the \n    image could be brightened or darkened.\n\n- **GPUImageSobelEdgeDetectionFilter**: Sobel edge detection, with edges \n  highlighted in white\n  - *texelWidth*: \n  - *texelHeight*: These parameters affect the visibility of the detected edges\n  - *edgeStrength*: Adjusts the dynamic range of the filter. Higher values \n    lead to stronger edges, but can saturate the intensity colorspace. Default \n    is 1.0.\n\n- **GPUImagePrewittEdgeDetectionFilter**: Prewitt edge detection, with edges \n  highlighted in white\n  - *texelWidth*: \n  - *texelHeight*: These parameters affect the visibility of the detected edges\n  - *edgeStrength*: Adjusts the dynamic range of the filter. Higher values \n    lead to stronger edges, but can saturate the intensity colorspace. Default \n    is 1.0.\n\n- **GPUImageThresholdEdgeDetectionFilter**: Performs Sobel edge detection, but \n  applies a threshold instead of giving gradual strength values\n  - *texelWidth*: \n  - *texelHeight*: These parameters affect the visibility of the detected edges\n  - *edgeStrength*: Adjusts the dynamic range of the filter. Higher values \n    lead to stronger edges, but can saturate the intensity colorspace. Default \n    is 1.0.\n  - *threshold*: Any edge above this threshold will be black, and anything \n    below white. Ranges from 0.0 to 1.0, with 0.8 as the default\n\n- **GPUImageCannyEdgeDetectionFilter**: This uses the full Canny process to \n  highlight one-pixel-wide edges\n  - *texelWidth*: \n  - *texelHeight*: These parameters affect the visibility of the detected edges\n  - *blurRadiusInPixels*: The underlying blur radius for the Gaussian blur. \n    Default is 2.0.\n  - *blurTexelSpacingMultiplier*: The underlying blur texel spacing multiplier. \n    Default is 1.0.\n  - *upperThreshold*: Any edge with a gradient magnitude above this threshold \n    will pass and show up in the final result. Default is 0.4.\n  - *lowerThreshold*: Any edge with a gradient magnitude below this threshold \n    will fail and be removed from the final result. Default is 0.1.\n\n- **GPUImageHarrisCornerDetectionFilter**: Runs the Harris corner detection \n  algorithm on an input image, and produces an image with those corner points \n  as white pixels and everything else black. The `CornersDetectedHandler` can \n  be set, and you will be provided with a list of corners (in normalized 0..1 \n  X, Y coordinates) within that callback for whatever additional operations \n  you want to perform.\n  - *blurRadiusInPixels*: The radius of the underlying Gaussian blur. The default \n    is 2.0.\n  - *sensitivity*: An internal scaling factor applied to adjust the dynamic \n    range of the cornerness maps generated in the filter. The default is 5.0.\n  - *threshold*: The threshold at which a point is detected as a corner. This \n    can vary significantly based on the size, lighting conditions, and iOS \n    device camera type, so it might take a little experimentation to get right \n    for your cases. Default is 0.20.\n\n- **GPUImageNobleCornerDetectionFilter**: Runs the Noble variant on the Harris \n  corner detector. It behaves as described above for the Harris detector.\n  - *blurRadiusInPixels*: The radius of the underlying Gaussian blur. The default \n    is 2.0.\n  - *sensitivity*: An internal scaling factor applied to adjust the dynamic range \n    of the cornerness maps generated in the filter. The default is 5.0.\n  - *threshold*: The threshold at which a point is detected as a corner. This can \n    vary significantly based on the size, lighting conditions, and iOS device \n    camera type, so it might take a little experimentation to get right for \n    your cases. Default is 0.2.\n\n- **GPUImageShiTomasiCornerDetectionFilter**: Runs the Shi-Tomasi feature \n  detector. It behaves as described above for the Harris detector.\n  - *blurRadiusInPixels*: The radius of the underlying Gaussian blur. The \n    default is 2.0.\n  - *sensitivity*: An internal scaling factor applied to adjust the dynamic \n    range of the cornerness maps generated in the filter. The default is 1.5.\n  - *threshold*: The threshold at which a point is detected as a corner. This \n    can vary significantly based on the size, lighting conditions, and iOS \n    device camera type, so it might take a little experimentation to get right \n    for your cases. Default is 0.2.\n\n- **GPUImageNonMaximumSuppressionFilter**: Currently used only as part of the \n  Harris corner detection filter, this will sample a 1-pixel box around each \n  pixel and determine if the center pixel\u0027s red channel is the maximum in \n  that area. If it is, it stays. If not, it is set to 0 for all color \n  components.\n\n- **GPUImageXYDerivativeFilter**: An internal component within the Harris corner \n  detection filter, this calculates the squared difference between the pixels \n  to the left and right of this one, the squared difference of the pixels \n  above and below this one, and the product of those two differences.\n\n- **GPUImageCrosshairGenerator**: This draws a series of crosshairs on an image, \n  most often used for identifying machine vision features. It does not take in \n  a standard image like other filters, but a series of points in its \n  `RenderCrosshairs` method, which does the actual drawing. You will need to \n  force this filter to render at the particular output size you need.\n  - *crosshairWidth*: The width, in pixels, of the crosshairs to be \n    drawn onscreen.\n\n- **GPUImageDilationFilter**: This performs an image dilation operation, where \n  the maximum intensity of the red channel in a rectangular neighborhood is \n  used for the intensity of this pixel. The radius of the rectangular area \n  to sample over is specified on initialization, with a range of 1-4 pixels. \n  This is intended for use with grayscale images, and it expands bright regions.\n\n- **GPUImageRGBDilationFilter**: This is the same as the `GPUImageDilationFilter`, \n  except that this acts on all color channels, not just the red channel.\n\n- **GPUImageErosionFilter**: This performs an image erosion operation, where the \n  minimum intensity of the red channel in a rectangular neighborhood is used for \n  the intensity of this pixel. The radius of the rectangular area to sample over \n  is specified on initialization, with a range of 1-4 pixels. This is intended \n  for use with grayscale images, and it expands dark regions.\n\n- **GPUImageRGBErosionFilter**: This is the same as the `GPUImageErosionFilter`, \n  except that this acts on all color channels, not just the red channel.\n\n- **GPUImageOpeningFilter**: This performs an erosion on the red channel of an \n  image, followed by a dilation of the same radius. The radius is set on \n  initialization, with a range of 1-4 pixels. This filters out smaller bright \n  regions.\n\n- **GPUImageRGBOpeningFilter**: This is the same as the `GPUImageOpeningFilter`, \n  except that this acts on all color channels, not just the red channel.\n\n- **GPUImageClosingFilter**: This performs a dilation on the red channel of an \n  image, followed by an erosion of the same radius. The radius is set on \n  initialization, with a range of 1-4 pixels. This filters out smaller dark \n  regions.\n\n- **GPUImageRGBClosingFilter**: This is the same as the `GPUImageClosingFilter`, \n  except that this acts on all color channels, not just the red channel.\n\n- **GPUImageLocalBinaryPatternFilter**: This performs a comparison of intensity \n  of the red channel of the 8 surrounding pixels and that of the central one, \n  encoding the comparison results in a bit string that becomes this pixel \n  intensity. The least-significant bit is the top-right comparison, going \n  counterclockwise to end at the right comparison as the most significant bit.\n\n- **GPUImageLowPassFilter**: This applies a low pass filter to incoming video \n  frames. This basically accumulates a weighted rolling average of previous \n  frames with the current ones as they come in. This can be used to denoise \n  video, add motion blur, or be used to create a high pass filter.\n  - *filterStrength*: This controls the degree by which the previous accumulated \n    frames are blended with the current one. This ranges from 0.0 to 1.0, with \n    a default of 0.5.\n\n- **GPUImageHighPassFilter**: This applies a high pass filter to incoming video \n  frames. This is the inverse of the low pass filter, showing the difference \n  between the current frame and the weighted rolling average of previous ones. \n  This is most useful for motion detection.\n  - *filterStrength*: This controls the degree by which the previous accumulated \n    frames are blended and then subtracted from the current one. This ranges \n    from 0.0 to 1.0, with a default of 0.5.\n\n- **GPUImageMotionDetector**: This is a motion detector based on a high-pass \n  filter. You set the `MotionDetectionHandler` and on every incoming frame it will \n  give you the centroid of any detected movement in the scene (in normalized \n  X,Y coordinates) as well as an intensity of motion for the scene.\n  - *lowPassFilterStrength*: This controls the strength of the low pass filter \n    used behind the scenes to establish the baseline that incoming frames are \n    compared with. This ranges from 0.0 to 1.0, with a default of 0.5.\n\n- **GPUImageHoughTransformLineDetector**: Detects lines in the image using a \n  Hough transform into parallel coordinate space. This approach is based \n  entirely on the PC lines process developed by the Graph@FIT research group \n  at the Brno University of Technology and described in their publications: \n  [M. Dubská, J. Havel, and A. Herout. Real-Time Detection of Lines using \n  Parallel Coordinates and OpenGL. Proceedings of SCCG 2011, Bratislava, \n  SK, p. 7][4] and [M. Dubská, J. Havel, and A. Herout. PClines — Line \n  detection using parallel coordinates. 2011 IEEE Conference on Computer \n  Vision and Pattern Recognition (CVPR), p. 1489- 1494][5].\n  - *edgeThreshold*: A threshold value for which a point is detected as belonging \n    to an edge for determining lines. Default is 0.9.\n  - *lineDetectionThreshold*: A threshold value for which a local maximum is \n    detected as belonging to a line in parallel coordinate space. Default is 0.20.\n  - *linesDetectedHandler*: This handler is called on the detection of lines, \n    usually on every processed frame. A C array containing normalized slopes \n    and intercepts in m, b pairs (y=mx+b) is passed in, along with a count of \n    the number of lines detected and the current timestamp of the video frame.\n\n- **GPUImageLineGenerator**: A helper class that generates lines which can \n  overlay the scene. The color of these lines can be adjusted using \n  `SetLineColor`\n  - *lineWidth*: The width of the lines, in pixels, with a default of 1.0.\n\n- **GPUImageMotionBlurFilter**: Applies a directional motion blur to an image\n  - *blurSize*: A multiplier for the blur size, ranging from 0.0 on up, with a \n    default of 1.0\n  - *blurAngle*: The angular direction of the blur, in degrees. 0 degrees by \n    default.\n\n- **GPUImageZoomBlurFilter**: Applies a directional motion blur to an image\n  - *blurSize*: A multiplier for the blur size, ranging from 0.0 on up, with a \n    default of 1.0\n  - *blurCenter*: The normalized center of the blur. (0.5, 0.5) by default\n\n### Blending Modes\n\n- **GPUImageChromaKeyBlendFilter**: Selectively replaces a color in the first \n  image with the second image\n  - *thresholdSensitivity*: How close a color match needs to exist to the target \n    color to be replaced (default of 0.4)\n  - *smoothing*: How smoothly to blend for the color match (default of 0.1)\n\n- **GPUImageDissolveBlendFilter**: Applies a dissolve blend of two images\n  - *mix*: The degree with which the second image overrides the first (0.0 - 1.0, \n    with 0.5 as the default)\n\n- **GPUImageMultiplyBlendFilter**: Applies a multiply blend of two images\n\n- **GPUImageAddBlendFilter**: Applies an additive blend of two images\n\n- **GPUImageSubtractBlendFilter**: Applies a subtractive blend of two images\n\n- **GPUImageDivideBlendFilter**: Applies a division blend of two images\n\n- **GPUImageOverlayBlendFilter**: Applies an overlay blend of two images\n\n- **GPUImageDarkenBlendFilter**: Blends two images by taking the minimum value \n  of each color component between the images\n\n- **GPUImageLightenBlendFilter**: Blends two images by taking the maximum value \n  of each color component between the images\n\n- **GPUImageColorBurnBlendFilter**: Applies a color burn blend of two images\n\n- **GPUImageColorDodgeBlendFilter**: Applies a color dodge blend of two images\n\n- **GPUImageScreenBlendFilter**: Applies a screen blend of two images\n\n- **GPUImageExclusionBlendFilter**: Applies an exclusion blend of two images\n\n- **GPUImageDifferenceBlendFilter**: Applies a difference blend of two images\n\n- **GPUImageHardLightBlendFilter**: Applies a hard light blend of two images\n\n- **GPUImageSoftLightBlendFilter**: Applies a soft light blend of two images\n\n- **GPUImageAlphaBlendFilter**: Blends the second image over the first, based \n  on the second\u0027s alpha channel\n  - *mix*: The degree with which the second image overrides the first (0.0 - \n    1.0, with 1.0 as the default)\n\n- **GPUImageSourceOverBlendFilter**: Applies a source over blend of two images\n\n- **GPUImageColorBurnBlendFilter**: Applies a color burn blend of two images\n\n- **GPUImageColorDodgeBlendFilter**: Applies a color dodge blend of two images\n\n- **GPUImageNormalBlendFilter**: Applies a normal blend of two images\n\n- **GPUImageColorBlendFilter**: Applies a color blend of two images\n\n- **GPUImageHueBlendFilter**: Applies a hue blend of two images\n\n- **GPUImageSaturationBlendFilter**: Applies a saturation blend of two images\n\n- **GPUImageLuminosityBlendFilter**: Applies a luminosity blend of two images\n\n- **GPUImageLinearBurnBlendFilter**: Applies a linear burn blend of two images\n\n- **GPUImagePoissonBlendFilter**: Applies a Poisson blend of two images\n  - *mix*: Mix ranges from 0.0 (only image 1) to 1.0 (only image 2 gradients), \n    with 1.0 as the normal level\n  - *numIterations*: The number of times to propagate the gradients. Crank this \n    up to 100 or even 1000 if you want to get anywhere near convergence.  Yes, \n    this will be slow.\n\n- **GPUImageMaskFilter**: Masks one image using another\n\n### Visual Effects\n\n- **GPUImagePixellateFilter**: Applies a pixellation effect on an image or video\n  - *fractionalWidthOfAPixel*: How large the pixels are, as a fraction of the \n    width and height of the image (0.0 - 1.0, default 0.05)\n\n- **GPUImagePolarPixellateFilter**: Applies a pixellation effect on an image or \n  video, based on polar coordinates instead of Cartesian ones\n  - *center*: The center about which to apply the pixellation, defaulting to \n    (0.5, 0.5)\n  - *pixelSize*: The fractional pixel size, split into width and height \n    components. The default is (0.05, 0.05)\n\n- **GPUImagePolkaDotFilter**: Breaks an image up into colored dots within a \n  regular grid\n  - *fractionalWidthOfAPixel*: How large the dots are, as a fraction of the \n    width and height of the image (0.0 - 1.0, default 0.05)\n  - *dotScaling*: What fraction of each grid space is taken up by a dot, \n    from 0.0 to 1.0 with a default of 0.9.\n\n- **GPUImageHalftoneFilter**: Applies a halftone effect to an image, like \n  news print\n  - *fractionalWidthOfAPixel*: How large the halftone dots are, as a \n    fraction of the width and height of the image (0.0 - 1.0, default 0.05)\n\n- **GPUImageCrosshatchFilter**: This converts an image into a black-and-white \n  crosshatch pattern\n  - *crossHatchSpacing*: The fractional width of the image to use as the \n    spacing for the crosshatch. The default is 0.03.\n  - *lineWidth*: A relative width for the crosshatch lines. The default is 0.003.\n\n- **GPUImageSketchFilter**: Converts video to look like a sketch. This is just \n  the Sobel edge detection filter with the colors inverted\n  - *texelWidth*: \n  - *texelHeight*: These parameters affect the visibility of the detected edges\n  - *edgeStrength*: Adjusts the dynamic range of the filter. Higher values lead \n    to stronger edges, but can saturate the intensity colorspace. Default is 1.0.\n\n- **GPUImageThresholdSketchFilter**: Same as the sketch filter, only the edges \n  are thresholded instead of being grayscale\n  - *texelWidth*: \n  - *texelHeight*: These parameters affect the visibility of the detected edges\n  - *edgeStrength*: Adjusts the dynamic range of the filter. Higher values lead \n    to stronger edges, but can saturate the intensity colorspace. Default is 1.0.\n  - *threshold*: Any edge above this threshold will be black, and anything below \n    white. Ranges from 0.0 to 1.0, with 0.8 as the default\n\n- **GPUImageToonFilter**: This uses Sobel edge detection to place a black border \n  around objects, and then it quantizes the colors present in the image to give \n  a cartoon-like quality to the image.\n  - *texelWidth*: \n  - *texelHeight*: These parameters affect the visibility of the detected edges\n  - *threshold*: The sensitivity of the edge detection, with lower values being \n    more sensitive. Ranges from 0.0 to 1.0, with 0.2 as the default\n  - *quantizationLevels*: The number of color levels to represent in the final \n    image. Default is 10.0\n\n- **GPUImageSmoothToonFilter**: This uses a similar process as the \n  `GPUImageToonFilter`, only it precedes the toon effect with a Gaussian blur \n  to smooth out noise.\n  - *texelWidth*: \n  - *texelHeight*: These parameters affect the visibility of the detected edges\n  - *blurRadiusInPixels*: The radius of the underlying Gaussian blur. The \n    default is 2.0.\n  - *threshold*: The sensitivity of the edge detection, with lower values \n    being more sensitive. Ranges from 0.0 to 1.0, with 0.2 as the default\n  - *quantizationLevels*: The number of color levels to represent in the \n    final image. Default is 10.0\n\n- **GPUImageEmbossFilter**: Applies an embossing effect on the image\n  - *intensity*: The strength of the embossing, from  0.0 to 4.0, with 1.0 as \n    the normal level\n\n- **GPUImagePosterizeFilter**: This reduces the color dynamic range into the \n  number of steps specified, leading to a cartoon-like simple shading of the \n  image.\n  - *colorLevels*: The number of color levels to reduce the image space to. \n    This ranges from 1 to 256, with a default of 10.\n\n- **GPUImageSwirlFilter**: Creates a swirl distortion on the image\n  - *radius*: The radius from the center to apply the distortion, with a default \n    of 0.5\n  - *center*: The center of the image (in normalized coordinates from 0 - 1.0) \n    about which to twist, with a default of (0.5, 0.5)\n  - *angle*: The amount of twist to apply to the image, with a default of 1.0\n\n- **GPUImageBulgeDistortionFilter**: Creates a bulge distortion on the image\n  - *radius*: The radius from the center to apply the distortion, with a default \n    of 0.25\n  - *center*: The center of the image (in normalized coordinates from 0 - 1.0) \n    about which to distort, with a default of (0.5, 0.5)\n  - *scale*: The amount of distortion to apply, from -1.0 to 1.0, with a default \n    of 0.5\n\n- **GPUImagePinchDistortionFilter**: Creates a pinch distortion of the image\n  - *radius*: The radius from the center to apply the distortion, with a default \n    of 1.0\n  - *center*: The center of the image (in normalized coordinates from 0 - 1.0) \n    about which to distort, with a default of (0.5, 0.5)\n  - *scale*: The amount of distortion to apply, from -2.0 to 2.0, with a default \n    of 1.0\n\n- **GPUImageStretchDistortionFilter**: Creates a stretch distortion of the image\n  - *center*: The center of the image (in normalized coordinates from 0 - 1.0) \n    about which to distort, with a default of (0.5, 0.5)\n\n- **GPUImageSphereRefractionFilter**: Simulates the refraction through a glass \n  sphere\n  - *center*: The center about which to apply the distortion, with a default of \n    (0.5, 0.5)\n  - *radius*: The radius of the distortion, ranging from 0.0 to 1.0, with a \n    default of 0.25\n  - *refractiveIndex*: The index of refraction for the sphere, with a default \n    of 0.71\n\n- **GPUImageGlassSphereFilter**: Same as the `GPUImageSphereRefractionFilter`, only \n  the image is not inverted and there\u0027s a little bit of frosting at the edges of \n  the glass\n  - *center*: The center about which to apply the distortion, with a default of \n    (0.5, 0.5)\n  - *radius*: The radius of the distortion, ranging from 0.0 to 1.0, with a \n    default of 0.25\n  - *refractiveIndex*: The index of refraction for the sphere, with a default \n    of 0.71\n\n- **GPUImageVignetteFilter**: Performs a vignetting effect, fading out the \n  image at the edges\n  - *x*:\n  - *y*: The directional intensity of the vignetting, with a default of x = \n    0.75, y = 0.5\n\n- **GPUImageKuwaharaFilter**: Kuwahara image abstraction, drawn from the work \n  of Kyprianidis, et. al. in their publication \"Anisotropic Kuwahara Filtering \n  on the GPU\" within the GPU Pro collection. This produces an oil-painting-like \n  image, but it is extremely computationally expensive, so it can take seconds \n  to render a frame on an iPad 2. This might be best used for still images.\n  - *radius*: In integer specifying the number of pixels out from the center \n    pixel to test when applying the filter, with a default of 4. A higher value \n    creates a more abstracted image, but at the cost of much greater processing \n    time.\n\n- **GPUImageKuwaharaRadius3Filter**: A modified version of the Kuwahara filter, \n  optimized to work over just a radius of three pixels\n\n- **GPUImagePerlinNoiseFilter**: Generates an image full of Perlin noise\n  - *colorStart*:\n  - *colorFinish*: The color range for the noise being generated\n  - *scale*: The scaling of the noise being generated\n\n- **GPUImageCGAColorspaceFilter**: Simulates the colorspace of a CGA monitor\n\n- **GPUImageMosaicFilter**: This filter takes an input tileset, the tiles must \n  ascend in luminance. It looks at the input image and replaces each display \n  tile with an input tile according to the luminance of that tile.  The idea \n  was to replicate the ASCII video filters seen in other apps, but the tileset \n  can be anything.\n  - *inputTileSize*:\n  - *numTiles*: \n  - *displayTileSize*:\n  - *colorOn*:\n\n- **GPUImageJFAVoronoiFilter**: Generates a Voronoi map, for use in a later stage.\n  - *sizeInPixels*: Size of the individual elements\n\n- **GPUImageVoronoiConsumerFilter**: Takes in the Voronoi map, and uses that \n  to filter an incoming image.\n  - *sizeInPixels*: Size of the individual elements\n\nYou can also easily write your own custom filters using the C-like OpenGL \nShading Language, as described above.\n\n[1]: http://www.sunsetlakesoftware.com/2010/10/22/gpu-accelerated-video-processing-mac-and-ios\n[2]: http://amatorka.deviantart.com/art/Amatorka-Action-2-121069631\n[3]: http://miss-etikate.deviantart.com/art/Photoshop-Action-15-120151961\n[4]: http://medusa.fit.vutbr.cz/public/data/papers/2011-SCCG-Dubska-Real-Time-Line-Detection-Using-PC-and-OpenGL.pdf\n[5]: http://medusa.fit.vutbr.cz/public/data/papers/2011-CVPR-Dubska-PClines.pdf\n\n[lookup]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup.png\n[lookup_amatorka]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup_amatorka.png\n[lookup_miss_etikate]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup_miss_etikate.png\n[lookup_soft_elegance_1]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup_soft_elegance_1.png\n[lookup_soft_elegance_2]: https://github.com/BradLarson/GPUImage/blob/0.1.7/framework/Resources/lookup_soft_elegance_2.png\n","Hash":"f102238c01731f225064fbc27cdfef7d","TargetPlatforms":["ios","ios-unified"],"TrialHash":null}